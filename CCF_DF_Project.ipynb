{"cells":[{"cell_type":"code","source":["import time\nfrom pyspark import SparkConf\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import ArrayType,IntegerType,MapType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cd55d94-c016-4b9f-ba2c-19cd39db5bda"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#attention fonctionnement différent de RDD / iic la fonction prend en argument la colonne spécifiée et non (key,value)\n#il faut passer Key en paramètre\nimport time\n#@udf(MapType(IntegerType(),IntegerType()))\n@udf(ArrayType(ArrayType(IntegerType())))\ndef DFreducerBis(key,value):\n    min=key\n    valuesList=[]\n    listoutput=[]\n#   global accu\n    for i in value:\n        if i <min: \n            min=i\n        valuesList.append(i)\n    if min < key:\n        listoutput.append((0,key,min))\n        for j in valuesList:\n            if min != j:\n              listoutput.append((1,j,min))\n#             accu.add(1)\n#   print(listoutput)\n    return listoutput"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b1a70e1-8e0f-4af4-94aa-a1e62e7c67aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Init=spark.read.csv(path + \"/facebook_combined.txt\",sep=\" \")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9b0bc49-5bf8-44b4-befb-a2a5f0318f6f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DF_Init.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ff9c8e0-3246-4b2f-b1d4-958a3d278967"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[18]: 88234","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[18]: 88234"]}}],"execution_count":0},{"cell_type":"code","source":["#essai programme complet n°1\n#formatage initial du fichier  / 88 234 edges\nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\n\n#Initialisation de la boucle\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate DF fichier de chiffres\n  ReduceDF=MapDF.withColumn(\"ListnewPairs\",DFreducerBis(\"key\",\"MapOutput\")).select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(3)])\n\n  compteur=ReduceDF.where(\"col[0]=1\").count()\n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la suppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[1]\",\"key\").withColumnRenamed(\"col[2]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7ff8d5c-37b1-4416-9bc2-21d84e201710"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"171914\n18583\n7052\n1507\n304\n0\narret\nc'est la fin; durée totale de : 51.70449709892273\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["171914\n18583\n7052\n1507\n304\n0\narret\nc'est la fin; durée totale de : 51.70449709892273\n"]}}],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Prep=spark.read.csv(path + \"/HR_edges.csv\",sep=\",\")\nDF_Init=DF_Prep.where(\"_c0 != 'node_1'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86eaa123-71b6-413c-b495-d0f4ba128c42"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DF_Init.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e07e719-694d-4a26-861c-62d6ff1f42a7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-----+\n|_c0|  _c1|\n+---+-----+\n|  0| 4076|\n|  0|29861|\n|  0|53717|\n|  0| 2382|\n|  0|39945|\n|  0|22224|\n|  0|17332|\n|  0|  226|\n|  0|30409|\n|  0|11699|\n+---+-----+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-----+\n|_c0|  _c1|\n+---+-----+\n|  0| 4076|\n|  0|29861|\n|  0|53717|\n|  0| 2382|\n|  0|39945|\n|  0|22224|\n|  0|17332|\n|  0|  226|\n|  0|30409|\n|  0|11699|\n+---+-----+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#essai programme complet n°2\n#fichier  498 203 paires de edges\n#formatage initial du fichier \nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\n\n#Initialisation de la boucle\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate RDD fichier de chiffres\n  ReduceDF=MapDF.withColumn(\"ListnewPairs\",DFreducerBis(\"key\",\"MapOutput\")).select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(3)])\n\n  compteur=ReduceDF.where(\"col[0]=1\").count()\n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la suppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[1]\",\"key\").withColumnRenamed(\"col[2]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46a509a2-eeea-4a7f-b69b-8d7ea6debd2c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"913541\n1450180\n1515978\n116067\n216\n0\narret\nc'est la fin; durée totale de : 144.9048490524292\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["913541\n1450180\n1515978\n116067\n216\n0\narret\nc'est la fin; durée totale de : 144.9048490524292\n"]}}],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Prep=spark.read.csv(path + \"/large_twitch_edges.csv\",sep=\",\")\nDF_Init=DF_Prep.where(\"_c0 != 'numeric_id_1'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfb6f7f1-4b1a-40b3-b591-b9b8b1b4ca6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DF_Init.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37fe0491-ae5f-4b89-b886-d229ce4afb4c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+\n|  _c0|   _c1|\n+-----+------+\n|98343|141493|\n|98343| 58736|\n|98343|140703|\n|98343|151401|\n|98343|157118|\n|98343|125430|\n|98343|  3635|\n|98343|   495|\n|98343|116648|\n|98343|  1679|\n+-----+------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+\n|  _c0|   _c1|\n+-----+------+\n|98343|141493|\n|98343| 58736|\n|98343|140703|\n|98343|151401|\n|98343|157118|\n|98343|125430|\n|98343|  3635|\n|98343|   495|\n|98343|116648|\n|98343|  1679|\n+-----+------+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["DF_Init.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0d88560-e1a5-409e-9ac3-9a50dcffee9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[23]: 6797557","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[23]: 6797557"]}}],"execution_count":0},{"cell_type":"code","source":["#essai programme complet n°4\n#chargement du fichier 6 797 557 paires de edges\n\n#formatage initial du fichier \nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\n\n#Initialisation de la boucle\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate RDD fichier de chiffres\n  ReduceDF=MapDF.withColumn(\"ListnewPairs\",DFreducerBis(\"key\",\"MapOutput\")).select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(3)])\n\n  compteur=ReduceDF.where(\"col[0]=1\").count()\n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la suppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[1]\",\"key\").withColumnRenamed(\"col[2]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1761a7c-fc08-4224-93ad-3038075a7078"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"13277898\n14353355\n1593964\n79434\n8\n0\narret\nc'est la fin; durée totale de : 694.192519903183\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["13277898\n14353355\n1593964\n79434\n8\n0\narret\nc'est la fin; durée totale de : 694.192519903183\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CCF_DF_Project","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3190659939319844}},"nbformat":4,"nbformat_minor":0}

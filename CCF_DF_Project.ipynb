{"cells":[{"cell_type":"code","source":["import time\nfrom pyspark import SparkConf\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import ArrayType,IntegerType,MapType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cd55d94-c016-4b9f-ba2c-19cd39db5bda"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#à vérifier mais probablement inutile\nfrom pyspark.sql.functions import split"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5268af89-b9f7-4ff1-8871-b490479c1d10"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#attention fonctionnement différent de RDD / iic la fonction prend en argument la colonne spécifiée et non (key,value)\n#il faut passer Key en paramètre\nimport time\n#@udf(MapType(IntegerType(),IntegerType()))\n@udf(ArrayType(ArrayType(IntegerType())))\ndef DFreducerBis(key,value):\n    min=key\n    valuesList=[]\n    listoutput=[]\n#   global accu\n    for i in value:\n        if i <min: \n            min=i\n        valuesList.append(i)\n    if min < key:\n        listoutput.append((0,key,min))\n        for j in valuesList:\n            if min != j:\n              listoutput.append((1,j,min))\n#             accu.add(1)\n#   print(listoutput)\n    return listoutput"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b1a70e1-8e0f-4af4-94aa-a1e62e7c67aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#2ieme méthode de comptage du nombre de paires créées par le reduce\n#cette méthode utilise le même code que la version RDD\n#attention fonctionnement différent de RDD / iic la fonction prend en argument la colonne spécifiée et non (key,value)\n#il faut passer Key en paramètre\nfrom pyspark.sql.types import ArrayType,IntegerType,MapType\n#@udf(MapType(IntegerType(),IntegerType()))\n@udf(ArrayType(ArrayType(StringType())))\ndef DFreducer(key,value):\n    min=key\n    valuesList=[]\n    listoutput=[]\n#    global accu\n    for i in value:\n        if i <min: \n            min=i\n        valuesList.append(i)\n    if min < key:\n        listoutput.append((key,min))\n        for j in valuesList:\n            if min != j:\n              listoutput.append((j,min))\n#              accu.add(1)\n#   print(listoutput)\n    return listoutput"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3890a8c2-266c-41c6-a599-e17f5cf7540d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#fichier web_Google.txt qui présente une difficulté de préparation des données :lignes à supprimer + séparateur /p\n#methode 1 en passant par un RDD (plus facile à manipuler) : solution trouvée sur internet\npath = \"/FileStore/tables/RDD/\" \nrdd_Prep = sc.textFile(path + \"/web_Google.txt\", use_unicode=\"False\")\n\nrdd_Init=rdd_Prep.filter(lambda x: \"#\" not in x)\\\n               .map(lambda x : x.split(\"\\t\"))\\\n               .map(lambda x : (int(x[0]), int(x[1])))\nDF_Init=spark.createDataFrame(rdd_Init)\n\nDF=DF_Init.withColumn(\"_1\",DF_Init[\"_1\"].cast(\"int\")).withColumnRenamed(\"_1\",\"key\")\\\n                     .withColumn(\"_2\",DF_Init[\"_2\"].cast(\"int\")).withColumnRenamed(\"_2\",\"value\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13838d85-10da-46be-8993-7e6c1106297b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#fichier web_Google.txt qui présente une difficulté de préparation des données :lignes à supprimer + séparateur /p\n#methode 1 en passant par un RDD (plus facile à manipuler) : solution trouvée sur internet\npath = \"/FileStore/tables/RDD/\" \nDF_Prep=spark.read.csv(path + \"/web_Google.txt\")\nDF_Init=DF_Prep.where(\"_c0 != '# Directed graph (each unordered pair of nodes is saved once): web-Google.txt '\")\\\n    .where((\"_c0 != '# Webgraph from the Google programming contest'\"))\\\n    .where((\"_c0 != '# Nodes: 875713 Edges: 5105039'\"))\\\n    .where((\"_c0 != '# FromNodeId\\tToNodeId'\"))\n\nDF = DF_Init.withColumn('key', split(DF_Init['_c0'], '\\t').getItem(0).cast(\"int\")) \\\n       .withColumn('value', split(DF_Init['_c0'], '\\t').getItem(1).cast(\"int\")).drop(col('_c0'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1ad3083-01b6-4a93-88c3-6708d8a077a0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Init=spark.read.csv(path + \"/facebook_combined.txt\",sep=\" \")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"629a2846-61d1-4259-a676-846701d51ac3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Prep=spark.read.csv(path + \"/HR_edges.csv\",sep=\",\")\nDF_Init=DF_Prep.where(\"_c0 != 'node_1'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86eaa123-71b6-413c-b495-d0f4ba128c42"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Prep=spark.read.csv(path + \"/large_twitch_edges.csv\",sep=\",\")\nDF_Init=DF_Prep.where(\"_c0 != 'numeric_id_1'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfb6f7f1-4b1a-40b3-b591-b9b8b1b4ca6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Prep=spark.read.csv(path + \"/artist_edges.csv\",sep=\",\")\nDF_Init=DF_Prep.where(\"_c0 != 'node_1'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"562eedae-041e-4072-8270-9653a8c5e01e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Init=spark.read.csv(path + \"/soc_pokec_relationships.txt\",sep=\"\\t\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"510ec2f5-3b31-4bb2-b72c-867355c5c598"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#programme complet avec le reducer bis (astuce de créer des triplets avec 0 ou 1 en fct de la création d'une pair ou non)\n#ligne de commande à supprimer dans le cas de web-google avec modification en DF\n#DF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n#                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\n\n#Initialisation de la boucle\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate RDD fichier de chiffres\n  ReduceDF=MapDF.withColumn(\"ListnewPairs\",DFreducerBis(\"key\",\"MapOutput\")).select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(3)])\n\n  compteur=ReduceDF.where(\"col[0]=1\").count()\n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la suppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[1]\",\"key\").withColumnRenamed(\"col[2]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb119f3b-2e37-476d-9ad1-60b404fe03ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"8552232\n4758451\n3278772\n3888454\n1905323\n86783\n1318\n0\narret\nc'est la fin; durée totale de : 1149.521509885788\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8552232\n4758451\n3278772\n3888454\n1905323\n86783\n1318\n0\narret\nc'est la fin; durée totale de : 1149.521509885788\n"]}}],"execution_count":0},{"cell_type":"code","source":["#autre méthode de calcul du compteur"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb791005-bdfd-4081-9269-9495fee9f156"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#essai programme avec l'autre méthode de calcul du nbre de paires créées par le reduce\n#voir les annexes pour les codes de vérification de fonctionnement\nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\n\n#Initialisation de la boucle\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate DF fichier de chiffres\n  ReduceDF_comptage=MapDF.withColumn(\"ListnewPairs\",DFreducer(\"key\",\"MapOutput\"))\n  ReduceDF=ReduceDF_comptage.select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(2)])\n\n  compteur=ReduceDF_comptage.withColumn(\"minimum_array\",array_min(\"MapOutput\"))\\\n    .filter(col('key') > col('minimum_array')).withColumn(\"compteur\",size(\"MapOutput\")-1).agg(sum(col('compteur'))).collect()[0][0] \n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la su#ppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[0]\",\"key\").withColumnRenamed(\"col[1]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c68110fa-1a1f-4c27-9914-c4a6e5761863"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"171914\n18733\n6516\n1658\n0\narret\nc'est la fin; durée totale de : 38.1501681804657\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["171914\n18733\n6516\n1658\n0\narret\nc'est la fin; durée totale de : 38.1501681804657\n"]}}],"execution_count":0},{"cell_type":"code","source":["# annexes / divers codes pour essais et contrôles"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7d57dd7-0599-476f-b82e-cd502e45c701"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Initialisation de la boucle / save du cas google - àmettre en annexe\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate RDD fichier de chiffres\n  ReduceDF=MapDF.withColumn(\"ListnewPairs\",DFreducerBis(\"key\",\"MapOutput\")).select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(3)])\n\n  compteur=ReduceDF.where(\"col[0]=1\").count()\n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la suppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[1]\",\"key\").withColumnRenamed(\"col[2]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b242a657-a5b4-45f5-95c2-74a15b7b9e48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"8552232\n4758451\n3278772\n3888454\n1905323\n86783\n1318\n0\narret\nc'est la fin; durée totale de : 1355.478346824646\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8552232\n4758451\n3278772\n3888454\n1905323\n86783\n1318\n0\narret\nc'est la fin; durée totale de : 1355.478346824646\n"]}}],"execution_count":0},{"cell_type":"code","source":["#zone de contrôle préparation contrôle de fonctionnement de la version bis du compteur / à supprimer à la fin\nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\nMapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\nReduceDF_comptage=MapDF.withColumn(\"ListnewPairs\",DFreducer(\"key\",\"MapOutput\"))\nReduceDF=ReduceDF_comptage.select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(2)])\nReduceDF_comptage.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"391957b3-9c71-4a9d-b860-fcade99d8f2f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#zone de vérification du fonctionnement du compteur pour récupérer le décompte dans une variable python / à supprimer à la fin \ncompteur=ReduceDF_comptage.withColumn(\"minimum_array\",array_min(\"MapOutput\")).filter(col('key') > col('minimum_array'))\\\n.withColumn(\"compteur\",size(\"MapOutput\")-1).agg(sum(col('compteur'))).collect()[0][0] \nprint(compteur)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a880c882-00d7-403a-bae6-a24611a2ed3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"171914\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["171914\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CCF_DF_Project","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3190659939319844}},"nbformat":4,"nbformat_minor":0}

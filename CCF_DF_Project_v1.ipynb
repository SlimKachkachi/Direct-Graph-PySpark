{"cells":[{"cell_type":"code","source":["import time\nfrom pyspark import SparkConf\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import ArrayType,IntegerType,MapType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cd55d94-c016-4b9f-ba2c-19cd39db5bda"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#attention fonctionnement différent de RDD / iic la fonction prend en argument la colonne spécifiée et non (key,value)\n#il faut passer Key en paramètre\nimport time\n#@udf(MapType(IntegerType(),IntegerType()))\n@udf(ArrayType(ArrayType(IntegerType())))\ndef DFreducerBis(key,value):\n    min=key\n    valuesList=[]\n    listoutput=[]\n#   global accu\n    for i in value:\n        if i <min: \n            min=i\n        valuesList.append(i)\n    if min < key:\n        listoutput.append((0,key,min))\n        for j in valuesList:\n            if min != j:\n              listoutput.append((1,j,min))\n#             accu.add(1)\n#   print(listoutput)\n    return listoutput"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b1a70e1-8e0f-4af4-94aa-a1e62e7c67aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Init=spark.read.csv(path + \"/facebook_combined.txt\",sep=\" \")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9b0bc49-5bf8-44b4-befb-a2a5f0318f6f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DF_Init.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ff9c8e0-3246-4b2f-b1d4-958a3d278967"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[18]: 88234","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[18]: 88234"]}}],"execution_count":0},{"cell_type":"code","source":["#essai programme complet n°1\n#formatage initial du fichier  / 88 234 edges\nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\n\n#Initialisation de la boucle\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate DF fichier de chiffres\n  ReduceDF=MapDF.withColumn(\"ListnewPairs\",DFreducerBis(\"key\",\"MapOutput\")).select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(3)])\n\n  compteur=ReduceDF.where(\"col[0]=1\").count()\n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la suppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[1]\",\"key\").withColumnRenamed(\"col[2]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7ff8d5c-37b1-4416-9bc2-21d84e201710"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"171914\n18583\n7052\n1507\n304\n0\narret\nc'est la fin; durée totale de : 38.134750843048096\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["171914\n18583\n7052\n1507\n304\n0\narret\nc'est la fin; durée totale de : 38.134750843048096\n"]}}],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Prep=spark.read.csv(path + \"/HR_edges.csv\",sep=\",\")\nDF_Init=DF_Prep.where(\"_c0 != 'node_1'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86eaa123-71b6-413c-b495-d0f4ba128c42"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DF_Init.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e07e719-694d-4a26-861c-62d6ff1f42a7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-----+\n|_c0|  _c1|\n+---+-----+\n|  0| 4076|\n|  0|29861|\n|  0|53717|\n|  0| 2382|\n|  0|39945|\n|  0|22224|\n|  0|17332|\n|  0|  226|\n|  0|30409|\n|  0|11699|\n+---+-----+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-----+\n|_c0|  _c1|\n+---+-----+\n|  0| 4076|\n|  0|29861|\n|  0|53717|\n|  0| 2382|\n|  0|39945|\n|  0|22224|\n|  0|17332|\n|  0|  226|\n|  0|30409|\n|  0|11699|\n+---+-----+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#essai programme complet n°2\n#fichier  498 203 paires de edges\n#formatage initial du fichier \nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\n\n#Initialisation de la boucle\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate RDD fichier de chiffres\n  ReduceDF=MapDF.withColumn(\"ListnewPairs\",DFreducerBis(\"key\",\"MapOutput\")).select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(3)])\n\n  compteur=ReduceDF.where(\"col[0]=1\").count()\n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la suppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[1]\",\"key\").withColumnRenamed(\"col[2]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46a509a2-eeea-4a7f-b69b-8d7ea6debd2c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"913541\n1450180\n1515978\n116067\n216\n0\narret\nc'est la fin; durée totale de : 144.9048490524292\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["913541\n1450180\n1515978\n116067\n216\n0\narret\nc'est la fin; durée totale de : 144.9048490524292\n"]}}],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Prep=spark.read.csv(path + \"/large_twitch_edges.csv\",sep=\",\")\nDF_Init=DF_Prep.where(\"_c0 != 'numeric_id_1'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfb6f7f1-4b1a-40b3-b591-b9b8b1b4ca6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DF_Init.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37fe0491-ae5f-4b89-b886-d229ce4afb4c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+\n|  _c0|   _c1|\n+-----+------+\n|98343|141493|\n|98343| 58736|\n|98343|140703|\n|98343|151401|\n|98343|157118|\n|98343|125430|\n|98343|  3635|\n|98343|   495|\n|98343|116648|\n|98343|  1679|\n+-----+------+\nonly showing top 10 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+\n|  _c0|   _c1|\n+-----+------+\n|98343|141493|\n|98343| 58736|\n|98343|140703|\n|98343|151401|\n|98343|157118|\n|98343|125430|\n|98343|  3635|\n|98343|   495|\n|98343|116648|\n|98343|  1679|\n+-----+------+\nonly showing top 10 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["DF_Init.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0d88560-e1a5-409e-9ac3-9a50dcffee9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[23]: 6797557","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[23]: 6797557"]}}],"execution_count":0},{"cell_type":"code","source":["#essai programme complet n°4\n#chargement du fichier 6 797 557 paires de edges\n\n#formatage initial du fichier \nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\n\n#Initialisation de la boucle\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate RDD fichier de chiffres\n  ReduceDF=MapDF.withColumn(\"ListnewPairs\",DFreducerBis(\"key\",\"MapOutput\")).select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(3)])\n\n  compteur=ReduceDF.where(\"col[0]=1\").count()\n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la suppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[1]\",\"key\").withColumnRenamed(\"col[2]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1761a7c-fc08-4224-93ad-3038075a7078"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"13277898\n14353355\n1593964\n79434\n8\n0\narret\nc'est la fin; durée totale de : 694.192519903183\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["13277898\n14353355\n1593964\n79434\n8\n0\narret\nc'est la fin; durée totale de : 694.192519903183\n"]}}],"execution_count":0},{"cell_type":"code","source":["#2ieme méthode de comptage du nombre de paires créées par le reduce\n#cette méthode utilise le même code que la version RDD\n#attention fonctionnement différent de RDD / iic la fonction prend en argument la colonne spécifiée et non (key,value)\n#il faut passer Key en paramètre\nfrom pyspark.sql.types import ArrayType,IntegerType,MapType\n#@udf(MapType(IntegerType(),IntegerType()))\n@udf(ArrayType(ArrayType(StringType())))\ndef DFreducer(key,value):\n    min=key\n    valuesList=[]\n    listoutput=[]\n#    global accu\n    for i in value:\n        if i <min: \n            min=i\n        valuesList.append(i)\n    if min < key:\n        listoutput.append((key,min))\n        for j in valuesList:\n            if min != j:\n              listoutput.append((j,min))\n#              accu.add(1)\n#   print(listoutput)\n    return listoutput"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3890a8c2-266c-41c6-a599-e17f5cf7540d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path = \"/FileStore/tables/RDD/\" \nDF_Init=spark.read.csv(path + \"/facebook_combined.txt\",sep=\" \")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"629a2846-61d1-4259-a676-846701d51ac3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#zone de contrôle préparation contrôle de fonctionnement de la version bis du compteur / à supprimer à la fin\nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\nMapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\nReduceDF_comptage=MapDF.withColumn(\"ListnewPairs\",DFreducer(\"key\",\"MapOutput\"))\nReduceDF=ReduceDF_comptage.select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(2)])\nReduceDF_comptage.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"391957b3-9c71-4a9d-b860-fcade99d8f2f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#zone de vérification du fonctionnement du compteur pour récupérer le décompte dans une variable python / à supprimer à la fin \ncompteur=ReduceDF_comptage.withColumn(\"minimum_array\",array_min(\"MapOutput\")).filter(col('key') > col('minimum_array'))\\\n.withColumn(\"compteur\",size(\"MapOutput\")-1).agg(sum(col('compteur'))).collect()[0][0] \nprint(compteur)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a880c882-00d7-403a-bae6-a24611a2ed3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"171914\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["171914\n"]}}],"execution_count":0},{"cell_type":"code","source":["ReduceDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b73f473e-be69-481f-a13a-e6c7d0de9561"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+------+\n|col[0]|col[1]|\n+------+------+\n|     1|     0|\n|    48|     0|\n|    53|     0|\n|    54|     0|\n|    73|     0|\n|    88|     0|\n|    92|     0|\n|   119|     0|\n|   126|     0|\n|   133|     0|\n|   194|     0|\n|   236|     0|\n|   280|     0|\n|   299|     0|\n|   315|     0|\n|   322|     0|\n|   346|     0|\n|     2|     0|\n|    20|     0|\n|   115|     0|\n+------+------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+------+\n|col[0]|col[1]|\n+------+------+\n|     1|     0|\n|    48|     0|\n|    53|     0|\n|    54|     0|\n|    73|     0|\n|    88|     0|\n|    92|     0|\n|   119|     0|\n|   126|     0|\n|   133|     0|\n|   194|     0|\n|   236|     0|\n|   280|     0|\n|   299|     0|\n|   315|     0|\n|   322|     0|\n|   346|     0|\n|     2|     0|\n|    20|     0|\n|   115|     0|\n+------+------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#essai programme complet n°1 bis avec l'autre méthode de calcul du nbre de paires créées par le reduce\n#formatage initial du fichier  / 88 234 edges\nDF=DF_Init.withColumn(\"_c0\",DF_Init[\"_c0\"].cast(\"int\")).withColumnRenamed(\"_c0\",\"key\")\\\n                     .withColumn(\"_c1\",DF_Init[\"_c1\"].cast(\"int\")).withColumnRenamed(\"_c1\",\"value\")\n\n#Initialisation de la boucle\nboucle = True\nt0=time.time()\n\nwhile boucle == True:\n\n  #map du CFF\n  MapDF=DF.union(DF.select('value','key')).groupBy(\"key\").agg(collect_list(\"value\").alias(\"MapOutput\"))\n\n  #fonction reduce du CCF Iterate DF fichier de chiffres\n  ReduceDF_comptage=MapDF.withColumn(\"ListnewPairs\",DFreducer(\"key\",\"MapOutput\"))\n  ReduceDF=ReduceDF_comptage.select(explode(col('ListnewPairs'))).select([col(\"col\")[i] for i in range(2)])\n\n  compteur=ReduceDF_comptage.withColumn(\"minimum_array\",array_min(\"MapOutput\"))\\\n    .filter(col('key') > col('minimum_array')).withColumn(\"compteur\",size(\"MapOutput\")-1).agg(sum(col('compteur'))).collect()[0][0] \n  print(compteur)\n\n  ##suite #fonction Reduce de CFF Dedup sur base RDD liste de chiffres pour vérifier la su#ppression des doublons de tuples\n  DF=ReduceDF.withColumnRenamed(\"col[0]\",\"key\").withColumnRenamed(\"col[1]\",\"value\").select(\"key\",\"value\").distinct()\n\n  if compteur == 0:\n    print(\"arret\")\n    boucle = False\n\nt1=time.time()\n#le round de python interfère l'opérateur round de pspark sql (qui s'applique à des colonnes)\nprint(\"c'est la fin; durée totale de :\",t1-t0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c68110fa-1a1f-4c27-9914-c4a6e5761863"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"171914\n18733\n6516\n1658\n0\narret\nc'est la fin; durée totale de : 38.1501681804657\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["171914\n18733\n6516\n1658\n0\narret\nc'est la fin; durée totale de : 38.1501681804657\n"]}}],"execution_count":0},{"cell_type":"code","source":["#comparaison avec le modèle 1e de compteur\n#171914\n#18583\n#7052\n#1507\n#304\n#0\n#arret\n#c'est la fin; durée totale de : 38.134750843048096"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7920c3a0-8a01-4b54-9b70-5c6ab0e8e7e0"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CCF_DF_Project","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3190659939319844}},"nbformat":4,"nbformat_minor":0}
